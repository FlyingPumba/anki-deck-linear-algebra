{
  "id": "25",
  "title": "Lesson 25: Projections and Projection Matrices",
  "lesson_title": "Projections and projection matrices",
  "objectives": [
    "Understand projection matrices and their properties",
    "Decompose vectors into parallel and perpendicular components",
    "Connect projections to least squares geometrically"
  ],
  "cards": [
    {
      "uid": "linear-algebra-25-001",
      "front": "What is a projection matrix?",
      "back": "A matrix \\( P \\) that projects vectors onto a subspace.<br><br>Applying \\( P \\) twice has no additional effect: \\( P^2 = P \\)<br><br>This property is called idempotence.",
      "tags": [
        "ch25",
        "projection-matrix",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-25-002",
      "front": "What are the two defining properties of an orthogonal projection matrix?",
      "back": "<ol><li>Idempotent: \\( P^2 = P \\)</li><li>Symmetric: \\( P = P^T \\)</li></ol><br>Any matrix satisfying both is an orthogonal projection onto some subspace.",
      "tags": [
        "ch25",
        "projection-matrix",
        "properties"
      ]
    },
    {
      "uid": "linear-algebra-25-003",
      "front": "What is the projection matrix onto the column space of \\( A \\)?",
      "back": "\\( P = A(A^T A)^{-1} A^T \\)<br><br>This projects any vector onto \\( \\text{Col}(A) \\).<br><br>Requires \\( A \\) to have linearly independent columns.",
      "tags": [
        "ch25",
        "projection-matrix",
        "formula"
      ]
    },
    {
      "uid": "linear-algebra-25-004",
      "front": "How do you decompose a vector \\( \\vec{x} \\) using a projection?",
      "back": "\\( \\vec{x} = \\vec{x}_\\parallel + \\vec{x}_\\perp \\)<br><br>Where:<br><br><ul><li>\\( \\vec{x}_\\parallel = P\\vec{x} \\) is in the subspace</li><li>\\( \\vec{x}_\\perp = (I - P)\\vec{x} \\) is orthogonal to the subspace</li></ul><br>The two components are perpendicular.",
      "tags": [
        "ch25",
        "decomposition",
        "formula"
      ]
    },
    {
      "uid": "linear-algebra-25-005",
      "front": "If \\( P \\) projects onto subspace \\( W \\), what does \\( I - P \\) do?",
      "back": "\\( I - P \\) projects onto \\( W^\\perp \\), the orthogonal complement of \\( W \\).<br><br>\\( (I - P) \\) is also a projection matrix: it's idempotent and symmetric.",
      "tags": [
        "ch25",
        "projection-matrix",
        "complement"
      ]
    },
    {
      "uid": "linear-algebra-25-006",
      "front": "What is the projection matrix onto a single vector \\( \\vec{u} \\)?",
      "back": "\\( P = \\frac{\\vec{u}\\vec{u}^T}{\\vec{u}^T\\vec{u}} \\)<br><br>This is a rank-1 matrix that projects onto the line spanned by \\( \\vec{u} \\).<br><br>If \\( \\vec{u} \\) is a unit vector: \\( P = \\vec{u}\\vec{u}^T \\)",
      "tags": [
        "ch25",
        "projection-matrix",
        "rank-1"
      ]
    },
    {
      "uid": "linear-algebra-25-007",
      "front": "How does least squares relate to projection geometrically?",
      "back": "The least-squares solution \\( \\hat{x} \\) makes \\( A\\hat{x} \\) the projection of \\( \\vec{b} \\) onto \\( \\text{Col}(A) \\).<br><br>\\( A\\hat{x} = P\\vec{b} \\) where \\( P = A(A^T A)^{-1}A^T \\)<br><br>The residual \\( \\vec{b} - A\\hat{x} \\) is orthogonal to \\( \\text{Col}(A) \\).",
      "tags": [
        "ch25",
        "least-squares",
        "geometry"
      ]
    },
    {
      "uid": "linear-algebra-25-008",
      "front": "What are the eigenvalues of a projection matrix?",
      "back": "Only 0 and 1.<br><br><ul><li>Eigenvalue 1: vectors in the subspace (unchanged by projection)</li><li>Eigenvalue 0: vectors in the orthogonal complement (mapped to zero)</li></ul><br>The rank equals the number of 1-eigenvalues.",
      "tags": [
        "ch25",
        "projection-matrix",
        "eigenvalues"
      ]
    },
    {
      "uid": "linear-algebra-25-009",
      "front": "What is the trace of a projection matrix \\( P \\)?",
      "back": "\\( \\text{trace}(P) = \\text{rank}(P) = \\dim(\\text{subspace}) \\)<br><br>Since eigenvalues are 0 and 1, and trace = sum of eigenvalues, the trace counts the dimension being projected onto.",
      "tags": [
        "ch25",
        "projection-matrix",
        "trace"
      ]
    },
    {
      "uid": "linear-algebra-25-010",
      "front": "Why are projections important in mechanistic interpretability?",
      "back": "They let us:<br><br><ul><li>Decompose activations into \"explained\" vs \"unexplained\" parts</li><li>Analyze feature subspaces in residual streams</li><li>Understand linear probes as projections</li><li>Interpret variance explained by directions</li></ul>",
      "tags": [
        "ch25",
        "projection",
        "mi-application"
      ]
    }
  ]
}
