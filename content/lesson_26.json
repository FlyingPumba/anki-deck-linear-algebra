{
  "id": "26",
  "title": "Lesson 26: Norms and Operator Norms",
  "lesson_title": "Norms, operator norms, and high-dimensional geometry",
  "objectives": [
    "Understand vector norms and their geometry",
    "Compute matrix norms",
    "Apply norms to bound transformations"
  ],
  "cards": [
    {
      "uid": "linear-algebra-26-001",
      "front": "What is a norm?",
      "back": "A function \\( ||\\cdot|| \\) measuring \"size\" that satisfies:<br><br><ol><li>\\( ||\\vec{x}|| \\geq 0 \\), with equality iff \\( \\vec{x} = \\vec{0} \\)</li><li>\\( ||c\\vec{x}|| = |c| \\cdot ||\\vec{x}|| \\) (scaling)</li><li>\\( ||\\vec{x} + \\vec{y}|| \\leq ||\\vec{x}|| + ||\\vec{y}|| \\) (triangle inequality)</li></ol>",
      "tags": [
        "ch26",
        "norm",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-26-002",
      "front": "What are the \\( \\ell_1 \\), \\( \\ell_2 \\), and \\( \\ell_\\infty \\) vector norms?",
      "back": "\\( ||\\vec{x}||_1 = \\sum_i |x_i| \\) (Manhattan distance)<br><br>\\( ||\\vec{x}||_2 = \\sqrt{\\sum_i x_i^2} \\) (Euclidean distance)<br><br>\\( ||\\vec{x}||_\\infty = \\max_i |x_i| \\) (max absolute value)",
      "tags": [
        "ch26",
        "vector-norm",
        "formulas"
      ]
    },
    {
      "uid": "linear-algebra-26-003",
      "front": "What do the unit balls look like for \\( \\ell_1 \\), \\( \\ell_2 \\), \\( \\ell_\\infty \\) in 2D?",
      "back": "\\( \\ell_1 \\): diamond (rotated square)<br><br>\\( \\ell_2 \\): circle<br><br>\\( \\ell_\\infty \\): square aligned with axes<br><br>In high dimensions, most volume of \\( \\ell_2 \\) ball concentrates near the surface.",
      "tags": [
        "ch26",
        "unit-ball",
        "geometry"
      ]
    },
    {
      "uid": "linear-algebra-26-004",
      "front": "What is the Frobenius norm of a matrix?",
      "back": "\\( ||A||_F = \\sqrt{\\sum_{i,j} a_{ij}^2} = \\sqrt{\\text{trace}(A^T A)} \\)<br><br>Treats the matrix as a long vector and takes its \\( \\ell_2 \\) norm.<br><br>Also equals \\( \\sqrt{\\sum_i \\sigma_i^2} \\) (sum of squared singular values).",
      "tags": [
        "ch26",
        "frobenius",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-26-005",
      "front": "What is the spectral norm (operator norm) of a matrix?",
      "back": "The operator norm induced by the \\( \\ell_2 \\) vector norm is:<br><br>\\( ||A||_2 = \\max_{\\vec{x} \\neq 0} \\frac{||A\\vec{x}||_2}{||\\vec{x}||_2} = \\sigma_{\\max}(A) \\)<br><br>It is the largest singular value and measures the maximum \"stretching\" the matrix can do in any direction.",
      "tags": [
        "ch26",
        "spectral-norm",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-26-006",
      "front": "What is submultiplicativity of matrix norms?",
      "back": "\\( ||AB|| \\leq ||A|| \\cdot ||B|| \\)<br><br>The norm of a product is at most the product of norms.<br><br>This lets us bound how transformations compound across layers.",
      "tags": [
        "ch26",
        "submultiplicativity",
        "property"
      ]
    },
    {
      "uid": "linear-algebra-26-007",
      "front": "How do you interpret \\( ||A||_2 \\) as a Lipschitz constant?",
      "back": "The linear map \\( \\vec{x} \\mapsto A\\vec{x} \\) satisfies:<br><br>\\( ||A\\vec{x} - A\\vec{y}||_2 \\leq ||A||_2 \\cdot ||\\vec{x} - \\vec{y}||_2 \\)<br><br>The spectral norm bounds how much distances can be amplified.",
      "tags": [
        "ch26",
        "lipschitz",
        "interpretation"
      ]
    },
    {
      "uid": "linear-algebra-26-008",
      "front": "What is the relationship between Frobenius and spectral norms?",
      "back": "\\( ||A||_2 \\leq ||A||_F \\leq \\sqrt{r} \\cdot ||A||_2 \\)<br><br>where \\( r = \\text{rank}(A) \\).<br><br>Frobenius is easier to compute; spectral is tighter for operator bounds.",
      "tags": [
        "ch26",
        "norm-comparison",
        "inequality"
      ]
    },
    {
      "uid": "linear-algebra-26-009",
      "front": "What is the nuclear norm (trace norm)?",
      "back": "\\( ||A||_* = \\sum_i \\sigma_i \\)<br><br>The sum of singular values.<br><br>Used as a convex surrogate for rank in optimization (encourages low-rank solutions).",
      "tags": [
        "ch26",
        "nuclear-norm",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-26-010",
      "front": "Why are norms important in mechanistic interpretability?",
      "back": "They let us:<br><br><ul><li>Bound amplification across layers (spectral norm)</li><li>Measure \"total weight\" of matrices (Frobenius)</li><li>Reason about which directions matter most</li><li>Compare magnitudes meaningfully across components</li></ul>",
      "tags": [
        "ch26",
        "norms",
        "mi-application"
      ]
    },
    {
      "uid": "linear-algebra-26-011",
      "front": "What are some quick mental bounds for \\( ||A||_2 \\)?",
      "back": "Easy upper bounds:<br><br><ul><li>\\( ||A||_2 \\leq ||A||_F = \\sqrt{\\sum_{i,j} a_{ij}^2} \\)</li><li>\\( ||A||_2 \\leq \\sqrt{||A||_1 \\, ||A||_\\infty} \\), where \\( ||A||_1 \\) is max column sum and \\( ||A||_\\infty \\) is max row sum</li></ul><br>Rule of thumb: if every row sums to at most \\( c \\) in absolute value, then \\( ||A||_2 \\lesssim c \\).",
      "tags": [
        "ch26",
        "spectral-norm",
        "bounds"
      ]
    }
  ]
}
