{
  "id": "21",
  "title": "Lesson 21: Gram-Schmidt Process",
  "lesson_title": "The Gram-Schmidt process",
  "objectives": [
    "Convert any basis to an orthogonal basis",
    "Apply the Gram-Schmidt algorithm",
    "Understand QR factorization"
  ],
  "cards": [
    {
      "uid": "linear-algebra-21-001",
      "front": "What does the Gram-Schmidt process do?",
      "back": "Converts any basis into an orthogonal (or orthonormal) basis for the same subspace.<br><br>Input: linearly independent vectors \\( \\{\\vec{x}_1, ..., \\vec{x}_k\\} \\)<br><br>Output: orthogonal vectors \\( \\{\\vec{v}_1, ..., \\vec{v}_k\\} \\) spanning the same space.",
      "tags": [
        "ch21",
        "gram-schmidt",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-21-002",
      "front": "What is the first step of Gram-Schmidt?",
      "back": "Set \\( \\vec{v}_1 = \\vec{x}_1 \\)<br><br>The first vector is kept as-is (or normalized if you want orthonormal).",
      "tags": [
        "ch21",
        "gram-schmidt",
        "algorithm"
      ]
    },
    {
      "uid": "linear-algebra-21-003",
      "front": "What is the Gram-Schmidt formula for \\( \\vec{v}_2 \\)?",
      "back": "\\( \\vec{v}_2 = \\vec{x}_2 - \\text{proj}_{\\vec{v}_1} \\vec{x}_2 \\)<br><br>\\( \\vec{v}_2 = \\vec{x}_2 - \\frac{\\vec{x}_2 \\cdot \\vec{v}_1}{\\vec{v}_1 \\cdot \\vec{v}_1} \\vec{v}_1 \\)<br><br>Subtract the component of \\( \\vec{x}_2 \\) in the direction of \\( \\vec{v}_1 \\).",
      "tags": [
        "ch21",
        "gram-schmidt",
        "formula"
      ]
    },
    {
      "uid": "linear-algebra-21-004",
      "front": "What is the general Gram-Schmidt formula for \\( \\vec{v}_k \\)?",
      "back": "\\( \\vec{v}_k = \\vec{x}_k - \\sum_{j=1}^{k-1} \\text{proj}_{\\vec{v}_j} \\vec{x}_k \\)<br><br>Subtract projections onto all previously computed orthogonal vectors.",
      "tags": [
        "ch21",
        "gram-schmidt",
        "formula"
      ]
    },
    {
      "uid": "linear-algebra-21-005",
      "front": "How do you get an orthonormal basis from Gram-Schmidt?",
      "back": "Normalize each vector after computing it:<br><br>\\( \\vec{u}_k = \\frac{\\vec{v}_k}{||\\vec{v}_k||} \\)<br><br>Or normalize all vectors at the end.",
      "tags": [
        "ch21",
        "gram-schmidt",
        "orthonormal"
      ]
    },
    {
      "uid": "linear-algebra-21-006",
      "front": "What is the QR factorization of a matrix?",
      "back": "\\( A = QR \\)<br><br>Where:<br><br><ul><li>\\( Q \\) is an \\( m \\times n \\) matrix with orthonormal columns</li><li>\\( R \\) is an \\( n \\times n \\) upper triangular matrix</li></ul><br>The columns of \\( Q \\) come from applying Gram-Schmidt to the columns of \\( A \\).",
      "tags": [
        "ch21",
        "qr-factorization",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-21-007",
      "front": "How do you find \\( R \\) in the QR factorization?",
      "back": "\\( R = Q^T A \\)<br><br>Since \\( Q \\) has orthonormal columns: \\( Q^T Q = I \\)<br><br>So from \\( A = QR \\), we get \\( Q^T A = Q^T Q R = R \\)",
      "tags": [
        "ch21",
        "qr-factorization",
        "computation"
      ]
    },
    {
      "uid": "linear-algebra-21-008",
      "front": "Why is QR factorization useful?",
      "back": "<ol><li>Solving least squares problems</li><li>Computing eigenvalues (QR algorithm)</li><li>Numerical stability in computations</li></ol><br>Orthogonal matrices preserve numerical precision.",
      "tags": [
        "ch21",
        "qr-factorization",
        "applications"
      ]
    }
  ]
}
