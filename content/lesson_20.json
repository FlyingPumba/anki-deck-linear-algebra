{
  "id": "20",
  "title": "Lesson 20: Orthogonality",
  "lesson_title": "Orthogonal bases and projections",
  "objectives": [
    "Understand orthogonal and orthonormal sets",
    "Compute orthogonal projections",
    "Work with orthogonal matrices"
  ],
  "cards": [
    {
      "uid": "linear-algebra-20-001",
      "front": "What does it mean for two vectors to be orthogonal?",
      "back": "Their dot product is zero: \\( \\vec{u} \\cdot \\vec{v} = 0 \\)<br><br>Geometrically, they are perpendicular (meet at a 90Â° angle).",
      "tags": [
        "ch20",
        "orthogonal",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-20-002",
      "front": "What is an orthogonal set of vectors?",
      "back": "A set where every pair of distinct vectors is orthogonal.<br><br>For all \\( i \\neq j \\): \\( \\vec{u}_i \\cdot \\vec{u}_j = 0 \\)",
      "tags": [
        "ch20",
        "orthogonal-set",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-20-003",
      "front": "What is an orthonormal set?",
      "back": "An orthogonal set where every vector is a unit vector (length 1).<br><br>\\( \\vec{u}_i \\cdot \\vec{u}_j = \\begin{cases} 1 & i = j \\\\ 0 & i \\neq j \\end{cases} \\)",
      "tags": [
        "ch20",
        "orthonormal",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-20-004",
      "front": "Why are orthogonal bases useful?",
      "back": "Coordinates are easy to compute using dot products:<br><br>If \\( \\{\\vec{u}_1, ..., \\vec{u}_n\\} \\) is orthogonal, then for any \\( \\vec{v} \\) in the span:<br><br>\\( c_i = \\frac{\\vec{v} \\cdot \\vec{u}_i}{\\vec{u}_i \\cdot \\vec{u}_i} \\)<br><br>No need to solve systems of equations.",
      "tags": [
        "ch20",
        "orthogonal-basis",
        "advantage"
      ]
    },
    {
      "uid": "linear-algebra-20-005",
      "front": "What is the orthogonal projection of \\( \\vec{v} \\) onto \\( \\vec{u} \\)?",
      "back": "\\( \\text{proj}_{\\vec{u}} \\vec{v} = \\frac{\\vec{v} \\cdot \\vec{u}}{\\vec{u} \\cdot \\vec{u}} \\vec{u} \\)<br><br>This is the component of \\( \\vec{v} \\) in the direction of \\( \\vec{u} \\).",
      "tags": [
        "ch20",
        "projection",
        "formula"
      ]
    },
    {
      "uid": "linear-algebra-20-006",
      "front": "What is the orthogonal projection onto a subspace?",
      "back": "The closest point in the subspace to a given vector.<br><br>If \\( W \\) has orthonormal basis \\( \\{\\vec{u}_1, ..., \\vec{u}_k\\} \\):<br><br>\\( \\text{proj}_W \\vec{v} = (\\vec{v} \\cdot \\vec{u}_1)\\vec{u}_1 + ... + (\\vec{v} \\cdot \\vec{u}_k)\\vec{u}_k \\)",
      "tags": [
        "ch20",
        "projection",
        "subspace"
      ]
    },
    {
      "uid": "linear-algebra-20-007",
      "front": "What is an orthogonal matrix?",
      "back": "A square matrix \\( Q \\) whose columns form an orthonormal set.<br><br>Equivalently: \\( Q^T Q = I \\), so \\( Q^{-1} = Q^T \\)",
      "tags": [
        "ch20",
        "orthogonal-matrix",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-20-008",
      "front": "What transformations do orthogonal matrices represent?",
      "back": "Transformations that preserve lengths and angles:<br><br><ul><li>Rotations</li><li>Reflections</li><li>Combinations of rotations and reflections</li></ul><br>\\( ||Q\\vec{x}|| = ||\\vec{x}|| \\) for all \\( \\vec{x} \\)",
      "tags": [
        "ch20",
        "orthogonal-matrix",
        "geometry"
      ]
    },
    {
      "uid": "linear-algebra-20-009",
      "front": "What is the determinant of an orthogonal matrix?",
      "back": "Either \\( +1 \\) or \\( -1 \\).<br><br><ul><li>\\( \\det(Q) = +1 \\): rotation (proper orthogonal)</li><li>\\( \\det(Q) = -1 \\): reflection (improper orthogonal)</li></ul>",
      "tags": [
        "ch20",
        "orthogonal-matrix",
        "determinant"
      ]
    },
    {
      "uid": "linear-algebra-20-010",
      "front": "What is the orthogonal complement of a subspace \\( W \\)?",
      "back": "The set of all vectors orthogonal to every vector in \\( W \\):<br><br>\\( W^\\perp = \\{\\vec{v} : \\vec{v} \\cdot \\vec{w} = 0 \\text{ for all } \\vec{w} \\in W\\} \\)<br><br>For a matrix \\( A \\):<br><ul><li>\\( (\\text{Row}(A))^\\perp = \\text{Null}(A) \\)</li><li>\\( (\\text{Col}(A))^\\perp = \\text{Null}(A^T) \\)</li></ul><br>Together with rank-nullity, these four spaces give the core \"mental schema\" of linear algebra.",
      "tags": [
        "ch20",
        "orthogonal-complement",
        "definition"
      ]
    }
  ]
}
