{
  "id": "24",
  "title": "Lesson 24: Singular Value Decomposition",
  "lesson_title": "Singular Value Decomposition (SVD)",
  "objectives": [
    "Understand the SVD factorization",
    "Compute singular values",
    "Apply SVD to various problems"
  ],
  "cards": [
    {
      "uid": "linear-algebra-24-001",
      "front": "What is the Singular Value Decomposition (SVD)?",
      "back": "Any \\( m \\times n \\) matrix \\( A \\) can be factored as:\n\n\\( A = U \\Sigma V^T \\)\n\nWhere:\n\n- \\( U \\) is \\( m \\times m \\) orthogonal\n- \\( \\Sigma \\) is \\( m \\times n \\) diagonal (singular values)\n- \\( V \\) is \\( n \\times n \\) orthogonal",
      "tags": [
        "ch24",
        "svd",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-24-002",
      "front": "What are singular values?",
      "back": "The diagonal entries \\( \\sigma_1, \\sigma_2, ... \\) of \\( \\Sigma \\), arranged in decreasing order.\n\nThey are the square roots of the eigenvalues of \\( A^T A \\) (or \\( AA^T \\)):\n\n\\( \\sigma_i = \\sqrt{\\lambda_i} \\)\n\nSingular values are always non-negative.",
      "tags": [
        "ch24",
        "singular-values",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-24-003",
      "front": "How do you find \\( V \\) in the SVD?",
      "back": "The columns of \\( V \\) are the eigenvectors of \\( A^T A \\), called right singular vectors.\n\nCompute the eigendecomposition of \\( A^T A = V \\Lambda V^T \\).",
      "tags": [
        "ch24",
        "svd",
        "computation"
      ]
    },
    {
      "uid": "linear-algebra-24-004",
      "front": "How do you find \\( U \\) in the SVD?",
      "back": "The columns of \\( U \\) are the eigenvectors of \\( AA^T \\), called left singular vectors.\n\nAlternatively: \\( \\vec{u}_i = \\frac{1}{\\sigma_i} A \\vec{v}_i \\) for non-zero singular values.",
      "tags": [
        "ch24",
        "svd",
        "computation"
      ]
    },
    {
      "uid": "linear-algebra-24-005",
      "front": "What is the geometric interpretation of SVD?",
      "back": "Every matrix transformation can be decomposed into:\n\n1. \\( V^T \\): rotate/reflect in the domain\n2. \\( \\Sigma \\): scale along coordinate axes\n3. \\( U \\): rotate/reflect in the codomain\n\nAny transformation is rotation-scaling-rotation.",
      "tags": [
        "ch24",
        "svd",
        "geometry"
      ]
    },
    {
      "uid": "linear-algebra-24-006",
      "front": "How does SVD relate to matrix rank?",
      "back": "The rank of \\( A \\) equals the number of non-zero singular values.\n\nThe non-zero singular values correspond to the \"important\" dimensions of the transformation.",
      "tags": [
        "ch24",
        "svd",
        "rank"
      ]
    },
    {
      "uid": "linear-algebra-24-007",
      "front": "What is the low-rank approximation using SVD?",
      "back": "The best rank-\\( k \\) approximation to \\( A \\) is:\n\n\\( A_k = \\sum_{i=1}^{k} \\sigma_i \\vec{u}_i \\vec{v}_i^T \\)\n\nKeep only the \\( k \\) largest singular values. This minimizes \\( ||A - A_k|| \\).",
      "tags": [
        "ch24",
        "low-rank",
        "approximation"
      ]
    },
    {
      "uid": "linear-algebra-24-008",
      "front": "How does SVD express the pseudoinverse?",
      "back": "\\( A^+ = V \\Sigma^+ U^T \\)\n\nWhere \\( \\Sigma^+ \\) inverts non-zero singular values and transposes the shape:\n\n\\( \\sigma_i^+ = \\begin{cases} 1/\\sigma_i & \\sigma_i \\neq 0 \\\\ 0 & \\sigma_i = 0 \\end{cases} \\)",
      "tags": [
        "ch24",
        "svd",
        "pseudoinverse"
      ]
    },
    {
      "uid": "linear-algebra-24-009",
      "front": "What is the condition number of a matrix?",
      "back": "\\( \\kappa(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\frac{\\sigma_1}{\\sigma_r} \\)\n\nMeasures sensitivity to numerical errors. Large condition number means the matrix is \"nearly singular\" and computations are unstable.",
      "tags": [
        "ch24",
        "condition-number",
        "numerical"
      ]
    },
    {
      "uid": "linear-algebra-24-010",
      "front": "What are applications of SVD?",
      "back": "1. Image compression (low-rank approximation)\n\n2. Noise reduction in data\n\n3. Principal Component Analysis (PCA)\n\n4. Recommender systems\n\n5. Solving least-squares problems\n\n6. Computing matrix rank numerically",
      "tags": [
        "ch24",
        "svd",
        "applications"
      ]
    },
    {
      "uid": "linear-algebra-24-011",
      "front": "How does SVD help interpret feature subspaces in mechanistic interpretability?",
      "back": "Think of \\( A \\) as a weight or activation matrix:\n\n- Columns of \\( V \\) are \"feature directions\" in input space\n- Columns of \\( U \\) show how those features appear across data points or layers\n- Large singular values pick out dominant, stable directions; tiny ones often correspond to noise or superposed features\n\nSVD on activations or weights reveals low-dimensional feature subspaces and superposition structure.",
      "tags": [
        "ch24",
        "svd",
        "mi-application"
      ]
    },
    {
      "uid": "linear-algebra-24-012",
      "front": "What are the key differences between eigenvalues and singular values?",
      "back": "Eigenvalues:\n\n- Only for square matrices\n- Can be negative or complex\n- Eigenvectors may not be orthogonal\n- Not every matrix is diagonalizable\n\nSingular values:\n\n- Work for any \\( m \\times n \\) matrix\n- Always real and non-negative\n- Singular vectors are always orthonormal\n- Every matrix has an SVD\n\nFor symmetric positive definite matrices, they coincide.",
      "tags": [
        "ch24",
        "eigenvalues",
        "singular-values",
        "comparison"
      ]
    },
    {
      "uid": "linear-algebra-24-013",
      "front": "How do you find the null space of a matrix using SVD?",
      "back": "A matrix \\( A \\) has a non-trivial null space iff one or more singular values are zero (or near-zero numerically).\n\nThe right singular vectors \\( \\vec{v}_i \\) corresponding to zero singular values span the null space.\n\nIn NumPy:\n\n```\nU, S, Vh = np.linalg.svd(A)\nnullspace = Vh[np.abs(S) <= eps].T\n```\n\nThis is more numerically stable than row reduction.",
      "tags": [
        "ch24",
        "svd",
        "null-space",
        "algorithm"
      ]
    },
    {
      "uid": "linear-algebra-24-014",
      "front": "Why are right singular vectors eigenvectors of \\( A^T A \\)?",
      "back": "From \\( A = U \\Sigma V^T \\):\n\n\\( A^T A = (U \\Sigma V^T)^T (U \\Sigma V^T) = V \\Sigma^T \\Sigma V^T \\)\n\nSo \\( A^T A \\, \\vec{v}_i = \\sigma_i^2 \\, \\vec{v}_i \\).\n\nThe columns of \\( V \\) are eigenvectors of \\( A^T A \\) with eigenvalues \\( \\sigma_i^2 \\).\n\nSimilarly, \\( A A^T = U \\Sigma \\Sigma^T U^T \\), so left singular vectors are eigenvectors of \\( A A^T \\).",
      "tags": [
        "ch24",
        "svd",
        "eigenvectors",
        "derivation"
      ]
    },
    {
      "uid": "linear-algebra-24-015",
      "front": "How do left and right singular vectors pair up?",
      "back": "For \\( \\sigma_i > 0 \\):\n\n\\( A \\vec{v}_i = \\sigma_i \\vec{u}_i \\)\n\\( A^T \\vec{u}_i = \\sigma_i \\vec{v}_i \\)\n\n\\( A \\) maps the right singular direction \\( \\vec{v}_i \\) to the left singular direction \\( \\vec{u}_i \\), scaled by \\( \\sigma_i \\).\n\nRight singular vectors live in input space (\\( \\mathbb{R}^n \\)); left singular vectors live in output space (\\( \\mathbb{R}^m \\)).",
      "tags": [
        "ch24",
        "svd",
        "singular-vectors"
      ]
    },
    {
      "uid": "linear-algebra-24-016",
      "front": "What happens to SVD when \\( A \\) is symmetric?",
      "back": "For symmetric \\( A = A^T \\):\n\n- Can choose \\( U = V \\)\n- Singular vectors coincide with eigenvectors of \\( A \\)\n- Singular values are \\( |\\lambda_i| \\) (absolute eigenvalues)\n\nThe eigendecomposition \\( A = Q \\Lambda Q^T \\) and SVD \\( A = U \\Sigma V^T \\) are closely related, differing only in sign handling.",
      "tags": [
        "ch24",
        "svd",
        "symmetric"
      ]
    },
    {
      "uid": "linear-algebra-24-017",
      "front": "Mental model: eigenvectors vs singular vectors?",
      "back": "Eigenvectors: directions preserved by \\( A \\) up to a (possibly negative/complex) scale \\( \\lambda \\).\n\nSingular vectors: directions that \\( A \\) stretches most cleanly, with guaranteed non-negative stretch \\( \\sigma_i \\).\n\nKey difference:\n\n- Eigenvectors: same space for input and output (square matrices only)\n- Singular vectors: right vectors in input space, left vectors in output space (any matrix)\n\nBoth are eigenvectors of symmetric PSD matrices: \\( V \\) from \\( A^T A \\), \\( U \\) from \\( A A^T \\).",
      "tags": [
        "ch24",
        "eigenvectors",
        "singular-vectors",
        "intuition"
      ]
    }
  ]
}
