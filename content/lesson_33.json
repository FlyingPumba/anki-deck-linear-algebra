{
  "id": "33",
  "title": "Lesson 33: Matrix Calculus for ML",
  "lesson_title": "Matrix calculus for ML",
  "objectives": [
    "Compute gradients of common matrix expressions",
    "Use the trace trick",
    "Understand Jacobians as linear maps"
  ],
  "cards": [
    {
      "uid": "linear-algebra-33-001",
      "front": "What is \\( \\frac{\\partial}{\\partial \\vec{x}} (\\vec{a}^T \\vec{x}) \\)?",
      "back": "\\( \\frac{\\partial}{\\partial \\vec{x}} (\\vec{a}^T \\vec{x}) = \\vec{a} \\)\n\nThe gradient of a linear function is the coefficient vector.",
      "tags": [
        "ch33",
        "gradient",
        "linear"
      ]
    },
    {
      "uid": "linear-algebra-33-002",
      "front": "What is \\( \\frac{\\partial}{\\partial \\vec{x}} (\\vec{x}^T A \\vec{x}) \\)?",
      "back": "\\( \\frac{\\partial}{\\partial \\vec{x}} (\\vec{x}^T A \\vec{x}) = (A + A^T) \\vec{x} \\)\n\nIf \\( A \\) is symmetric: \\( = 2A\\vec{x} \\)\n\nThis is the gradient of a quadratic form.",
      "tags": [
        "ch33",
        "gradient",
        "quadratic"
      ]
    },
    {
      "uid": "linear-algebra-33-003",
      "front": "What is \\( \\frac{\\partial}{\\partial \\vec{x}} ||A\\vec{x} - \\vec{b}||^2 \\)?",
      "back": "\\( \\frac{\\partial}{\\partial \\vec{x}} ||A\\vec{x} - \\vec{b}||^2 = 2A^T(A\\vec{x} - \\vec{b}) \\)\n\nSetting to zero gives normal equations: \\( A^T A \\vec{x} = A^T \\vec{b} \\)\n\nThis is the gradient for least squares.",
      "tags": [
        "ch33",
        "gradient",
        "least-squares"
      ]
    },
    {
      "uid": "linear-algebra-33-004",
      "front": "What is the trace trick for matrix derivatives?",
      "back": "Many scalar functions can be written as traces:\n\n\\( ||A||_F^2 = \\text{tr}(A^T A) \\)\n\\( \\vec{x}^T A \\vec{y} = \\text{tr}(\\vec{y} \\vec{x}^T A) \\)\n\nThen use: \\( \\frac{\\partial}{\\partial X} \\text{tr}(AXB) = A^T B^T \\)",
      "tags": [
        "ch33",
        "trace-trick",
        "technique"
      ]
    },
    {
      "uid": "linear-algebra-33-005",
      "front": "What is \\( \\frac{\\partial}{\\partial X} \\text{tr}(AX) \\)?",
      "back": "\\( \\frac{\\partial}{\\partial X} \\text{tr}(AX) = A^T \\)\n\nMore generally:\n\n\\( \\frac{\\partial}{\\partial X} \\text{tr}(X^T A) = A \\)",
      "tags": [
        "ch33",
        "gradient",
        "trace"
      ]
    },
    {
      "uid": "linear-algebra-33-006",
      "front": "What is the Jacobian of a vector-valued function?",
      "back": "For \\( f: \\mathbb{R}^n \\to \\mathbb{R}^m \\), the Jacobian is:\n\n\\( J_{ij} = \\frac{\\partial f_i}{\\partial x_j} \\)\n\nAn \\( m \\times n \\) matrix. It's the best linear approximation to \\( f \\) at a point.",
      "tags": [
        "ch33",
        "jacobian",
        "definition"
      ]
    },
    {
      "uid": "linear-algebra-33-007",
      "front": "How does the chain rule work with Jacobians?",
      "back": "For \\( f \\circ g \\):\n\n\\( J_{f \\circ g} = J_f \\cdot J_g \\)\n\nJacobians multiply like matrices.\n\nThis is backpropagation: multiply Jacobians backward through the computation graph.",
      "tags": [
        "ch33",
        "jacobian",
        "chain-rule"
      ]
    },
    {
      "uid": "linear-algebra-33-008",
      "front": "What is \\( \\frac{\\partial}{\\partial A} \\det(A) \\)?",
      "back": "\\( \\frac{\\partial}{\\partial A} \\det(A) = \\det(A) \\cdot A^{-T} = \\text{adj}(A)^T \\)\n\nwhere \\( \\text{adj}(A) \\) is the adjugate matrix.\n\nUseful for log-determinant: \\( \\frac{\\partial}{\\partial A} \\log \\det(A) = A^{-T} \\)",
      "tags": [
        "ch33",
        "gradient",
        "determinant"
      ]
    },
    {
      "uid": "linear-algebra-33-009",
      "front": "What is \\( \\frac{\\partial}{\\partial A} ||A||_F^2 \\)?",
      "back": "\\( \\frac{\\partial}{\\partial A} ||A||_F^2 = 2A \\)\n\nFrobenius norm squared is just sum of squared entries, so gradient is straightforward.",
      "tags": [
        "ch33",
        "gradient",
        "frobenius"
      ]
    },
    {
      "uid": "linear-algebra-33-010",
      "front": "Why is matrix calculus important for MI?",
      "back": "Understanding gradients helps:\n\n- Interpret what the model \"wants\" to change\n- Analyze linearized behavior around points\n- Understand optimization dynamics\n- Compute influence functions and attributions\n\nGradients are linear maps; linear algebra applies.",
      "tags": [
        "ch33",
        "matrix-calculus",
        "mi-application"
      ]
    },
    {
      "uid": "linear-algebra-33-011",
      "front": "How do you \"shape-check\" matrix calculus expressions?",
      "back": "Discipline for avoiding mistakes:\n\n- Always write down shapes (e.g., \\( A \\in \\mathbb{R}^{m \\times n} \\), \\( X \\in \\mathbb{R}^{m \\times n} \\))\n- Gradients have the same shape as the variable you differentiate with respect to\n- Any trace expression \\( \\text{tr}(AXB) \\) must be scalar: inner dimensions must match and result be \\( 1 \\times 1 \\)\n\nIf a gradient or trace has the wrong shape, the formula is wrong.",
      "tags": [
        "ch33",
        "matrix-calculus",
        "shape-checking"
      ]
    },
    {
      "uid": "linear-algebra-33-012",
      "front": "Consider \\( f(\\vec{x}) = \\text{ReLU}(W\\vec{x} + \\vec{b}) \\) where \\( \\vec{x} \\in \\mathbb{R}^5 \\), \\( \\vec{b} \\in \\mathbb{R}^3 \\), and \\( W \\) is \\( 3 \\times 5 \\).\n\nWhat is the Jacobian of \\( f \\)?",
      "back": "Using chain rule with \\( g(\\vec{x}) = W\\vec{x} + \\vec{b} \\):\n\n\\( J_f = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial \\vec{x}} \\)\n\n\\( \\frac{\\partial g}{\\partial \\vec{x}} = W \\)\n\n\\( \\frac{\\partial \\text{ReLU}}{\\partial g} = \\text{diag}(\\mathbf{1}_{g_i > 0}) \\) (diagonal matrix with 1 where \\( g_i > 0 \\), else 0)\n\nResult: \\( J_f \\) is \\( W \\) with rows zeroed out wherever \\( (W\\vec{x} + \\vec{b})_i \\leq 0 \\).\n\nShape: \\( 3 \\times 5 \\) (output dim \\( \\times \\) input dim).",
      "tags": [
        "ch33",
        "jacobian",
        "relu",
        "chain-rule"
      ]
    },
    {
      "uid": "linear-algebra-33-013",
      "front": "Where is the Jacobian of \\( f(\\vec{x}) = \\text{ReLU}(W\\vec{x} + \\vec{b}) \\) not well-defined?",
      "back": "The Jacobian is undefined wherever any element of \\( (W\\vec{x} + \\vec{b}) \\) equals exactly 0.\n\nReLU is not differentiable at 0: the left derivative is 0, the right derivative is 1.\n\nGeometrically, these are hyperplanes in input space where the output \"kinks.\"",
      "tags": [
        "ch33",
        "jacobian",
        "relu",
        "differentiability"
      ]
    },
    {
      "uid": "linear-algebra-33-014",
      "front": "How do we handle non-differentiability of ReLU at zero in practice?",
      "back": "Use a subgradient: choose any value in \\( [0, 1] \\) for the derivative at 0.\n\nConvention: most frameworks set \\( \\text{ReLU}'(0) = 0 \\).\n\nThis works because:\n\n- The set of inputs hitting exactly 0 has measure zero\n- Subgradient descent still converges for convex problems\n- In practice, floating point rarely lands exactly on 0",
      "tags": [
        "ch33",
        "subgradient",
        "relu",
        "practical"
      ]
    }
  ]
}
